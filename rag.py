import os
import shutil
import hashlib
import requests
import certifi
from bs4 import BeautifulSoup
from docx import Document
from PyPDF2 import PdfReader
from odf import text, teletype
from odf.opendocument import load
from elasticsearch import Elasticsearch
import time
import re
import json
import jieba.analyse
import opencc
from sentence_transformers import SentenceTransformer
import logging

# === logging è¨­å®š ===
logging.basicConfig(
    level=logging.WARNING,
    format="%(asctime)s %(levelname)s: %(message)s"
)

# === åƒæ•¸è¨­å®š ===
embedding_model = SentenceTransformer('shibing624/text2vec-base-chinese')
DOWNLOAD_FOLDER = "docs"
CHUNK_FOLDER = os.path.join(DOWNLOAD_FOLDER, "chunks")
TEXT_FOLDER = os.path.join(DOWNLOAD_FOLDER, "text")
SCU_URL = "https://web-ch.scu.edu.tw/secretary/web_page/1655"
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
ES_INDEX = "mydocs"

for folder in [DOWNLOAD_FOLDER, CHUNK_FOLDER, TEXT_FOLDER]:
    os.makedirs(folder, exist_ok=True)

es = Elasticsearch("http://localhost:9200")


# === ä¸‹è¼‰æ–‡ä»¶ ===
def download_files():
    if any(fname.startswith("doc_") for fname in os.listdir(DOWNLOAD_FOLDER)):
        logging.info("æœ¬åœ°å·²æœ‰ä¸‹è¼‰æª”æ¡ˆï¼Œè·³éä¸‹è¼‰éšæ®µã€‚")
        return
    logging.info(f"é–‹å§‹å¾ {SCU_URL} ä¸‹è¼‰æ–‡ä»¶...")
    try:
        response = requests.get(
            SCU_URL, headers=HEADERS, verify=certifi.where())
        if response.status_code != 200:
            logging.error(f"ä¸‹è¼‰ç¶²é å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}")
            return
    except Exception as e:
        logging.error(f"ä¸‹è¼‰ç¶²é å¤±æ•—ï¼ŒéŒ¯èª¤: {e}")
        return

    soup = BeautifulSoup(response.text, "html.parser")
    extensions = [".pdf", ".doc", ".docx", ".odt"]
    file_links = []
    for link in soup.find_all("a", href=True):
        href = link["href"].lower()
        if any(ext in href for ext in extensions) or ".ashx" in href:
            full_url = href if href.startswith(
                "http") else "https://web-ch.scu.edu.tw" + href
            file_links.append(full_url)

    logging.info(f"æ‰¾åˆ° {len(file_links)} ç­†æ–‡ä»¶é€£çµï¼Œé–‹å§‹ä¸‹è¼‰...")
    for i, url in enumerate(file_links, 1):
        ext = os.path.splitext(url)[1]
        if ".ashx" in ext or not ext:
            ext = ".pdf"
        filename = f"doc_{i}{ext}"
        filepath = os.path.join(DOWNLOAD_FOLDER, filename)
        if os.path.exists(filepath):
            logging.info(f"æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰ï¼š{filename}")
            continue
        try:
            resp = requests.get(url, headers=HEADERS, verify=False)
            with open(filepath, "wb") as f:
                f.write(resp.content)
            logging.info(f"â¬‡ï¸ å·²ä¸‹è¼‰ï¼š{filename}")
            time.sleep(0.5)
        except Exception as e:
            logging.error(f"ä¸‹è¼‰å¤±æ•—ï¼š{url}ï¼ŒéŒ¯èª¤ï¼š{e}")
    logging.info("ä¸‹è¼‰å®Œæˆã€‚\n")


# === åˆ†é¡èˆ‡è½‰æ–‡å­— ===
seen_hashes = set()
folders = {
    'pdf': os.path.join(DOWNLOAD_FOLDER, 'pdf'),
    'docx': os.path.join(DOWNLOAD_FOLDER, 'docx'),
    'odt': os.path.join(DOWNLOAD_FOLDER, 'odt'),
    'others': os.path.join(DOWNLOAD_FOLDER, 'others'),
    'text': TEXT_FOLDER
}
for p in folders.values():
    os.makedirs(p, exist_ok=True)


def get_file_hash(path):
    h = hashlib.md5()
    with open(path, 'rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            h.update(chunk)
    return h.hexdigest()


def extract_text_pdf(filepath):
    try:
        reader = PdfReader(filepath)
        return ''.join([page.extract_text() or '' for page in reader.pages]).strip()
    except Exception as e:
        logging.error(f"PDF è§£æéŒ¯èª¤ ({filepath}): {e}")
        return f"ã€PDF è§£æéŒ¯èª¤ã€‘: {e}"


def extract_text_docx(filepath):
    try:
        doc = Document(filepath)
        return '\n'.join(p.text for p in doc.paragraphs)
    except Exception as e:
        logging.error(f"DOCX è§£æéŒ¯èª¤ ({filepath}): {e}")
        return f"ã€DOCX è§£æéŒ¯èª¤ã€‘: {e}"


def extract_text_odt(filepath):
    try:
        odt_doc = load(filepath)
        texts = [teletype.extractText(p)
                 for p in odt_doc.getElementsByType(text.P)]
        return '\n'.join(texts)
    except Exception as e:
        logging.error(f"ODT è§£æéŒ¯èª¤ ({filepath}): {e}")
        return f"ã€ODT è§£æéŒ¯èª¤ã€‘: {e}"


def classify_and_extract():
    if len(os.listdir(TEXT_FOLDER)) > 0:
        logging.info("æœ¬åœ°å·²æœ‰è½‰æ–‡å­—æª”ï¼Œè·³éåˆ†é¡è½‰æ–‡å­—éšæ®µã€‚")
        return
    logging.info("é–‹å§‹åˆ†é¡ä¸¦è½‰æ–‡å­—...")
    for filename in os.listdir(DOWNLOAD_FOLDER):
        filepath = os.path.join(DOWNLOAD_FOLDER, filename)
        if not os.path.isfile(filepath):
            continue
        ext = filename.lower().split('.')[-1]
        file_hash = get_file_hash(filepath)
        if file_hash in seen_hashes:
            logging.info(f"ğŸ—‘ï¸ é‡è¤‡æª”æ¡ˆç•¥éï¼š{filename}")
            continue
        seen_hashes.add(file_hash)
        try:
            text_content = None
            if ext == 'pdf':
                dest = os.path.join(folders['pdf'], filename)
                shutil.move(filepath, dest)
                text_content = extract_text_pdf(dest)
            elif ext == 'docx':
                dest = os.path.join(folders['docx'], filename)
                shutil.move(filepath, dest)
                text_content = extract_text_docx(dest)
            elif ext == 'odt':
                dest = os.path.join(folders['odt'], filename)
                shutil.move(filepath, dest)
                text_content = extract_text_odt(dest)
            else:
                shutil.move(filepath, os.path.join(
                    folders['others'], filename))
            if text_content:
                txt_path = os.path.join(
                    folders['text'], os.path.splitext(filename)[0] + '.txt')
                with open(txt_path, 'w', encoding='utf-8') as f:
                    f.write(text_content)
                logging.info(f"ğŸ“„ è½‰æ–‡å­—å®Œæˆï¼š{txt_path}")
        except Exception as e:
            logging.warning(f"è§£æéŒ¯èª¤ï¼š{filename} â†’ {e}")
    logging.info("åˆ†é¡è½‰æ–‡å­—å®Œæˆã€‚\n")
# === åˆ‡æ®µ ===


def load_texts_from_folder(folder_path):
    texts = []
    for fname in os.listdir(folder_path):
        if fname.endswith('.txt'):
            with open(os.path.join(folder_path, fname), 'r', encoding='utf-8') as f:
                texts.append(f.read())
    return texts


def split_text(text, max_len=500, overlap=50):
    raw_sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›\n])', text)
    sentences = [s.strip() for s in raw_sentences if s.strip()]

    chunks = []
    current = ""
    for sentence in sentences:
        if len(current) + len(sentence) > max_len:
            chunks.append(current.strip())
            current = current[-overlap:]
        current += sentence
    if current:
        chunks.append(current.strip())

    return chunks


def chunk_texts():
    if len(os.listdir(CHUNK_FOLDER)) > 0:
        logging.info("æœ¬åœ°å·²æœ‰åˆ‡æ®µæª”ï¼Œè·³éåˆ‡æ®µéšæ®µã€‚")
        return
    logging.info("é–‹å§‹åˆ‡æ®µ...")
    all_texts = load_texts_from_folder(TEXT_FOLDER)
    all_chunks = []
    for text in all_texts:
        all_chunks.extend(split_text(text))
    for i, chunk in enumerate(all_chunks, 1):
        with open(f"{CHUNK_FOLDER}/chunk_{i}.txt", "w", encoding="utf-8") as f:
            f.write(chunk)
    logging.info(f"å·²åˆ‡å‡º {len(all_chunks)} å€‹æ®µè½ã€‚\n")


# === å»ºç«‹ Elasticsearch ç´¢å¼•åŠ Mapping ===
def create_es_index():
    logging.info("ğŸ”§ å»ºç«‹ Elasticsearch å‘é‡ç´¢å¼•...")
    mapping = {
        "mappings": {
            "properties": {
                "content": {"type": "text"},
                "keywords": {"type": "text"},
                "summary": {"type": "text"},
                "embedding": {
                    "type": "dense_vector",
                    "dims": 768,
                    "index": True,
                    "similarity": "cosine",
                }
            }
        }
    }

    if es.indices.exists(index=ES_INDEX):
        logging.info("Elasticsearch ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³éå»ºç«‹ç´¢å¼•éšæ®µã€‚")
        return
    es.indices.create(index=ES_INDEX, body=mapping)
    logging.info("âœ… Elasticsearch ç´¢å¼•å»ºç«‹å®Œæˆã€‚\n")


def get_embedding(text):
    try:
        embedding = embedding_model.encode(text, normalize_embeddings=True)
        return embedding.tolist()
    except Exception as e:
        logging.error(f"âš ï¸ ç”¢ç”Ÿ embedding å¤±æ•—ï¼š{e}")
        return None


def extract_keywords(text, top_k=5):
    return jieba.analyse.extract_tags(text, topK=top_k)


def extract_summary(text, ratio=0.3):
    try:
        from gensim.summarization import summarize
        return summarize(text, ratio=ratio)
    except Exception:
        return ""


def embed_and_index():
    chunk_files = sorted(os.listdir(CHUNK_FOLDER))
    existing_count = 0
    if es.indices.exists(index=ES_INDEX):
        existing_count = es.count(index=ES_INDEX)['count']
    if existing_count >= len(chunk_files) and len(chunk_files) > 0:
        logging.info("Elasticsearch å·²æœ‰è¶³å¤ è³‡æ–™ï¼Œè·³éç”¢ç”Ÿå‘é‡èˆ‡ç´¢å¼•éšæ®µã€‚")
        return
    logging.info("ğŸš€ é–‹å§‹ç”¢ç”Ÿå‘é‡ä¸¦å¯«å…¥ Elasticsearch...")
    for i, fname in enumerate(chunk_files, 1):
        path = os.path.join(CHUNK_FOLDER, fname)
        with open(path, "r", encoding="utf-8") as f:
            text = f.read().strip()
        if not text:
            logging.warning(f"âš ï¸ ç©ºç™½æ®µè½è·³éï¼š{fname}")
            continue
        embedding = get_embedding(text)
        if embedding is None:
            logging.error(f"âŒ å‘é‡ç”¢ç”Ÿå¤±æ•—ï¼Œè·³éï¼š{fname}")
            continue
        keywords = extract_keywords(text)
        summary = extract_summary(text)

        doc = {
            "content": text,
            "keywords": ", ".join(keywords),
            "summary": summary,
            "embedding": embedding
        }
        es.index(index=ES_INDEX, id=i, document=doc)
        logging.info(f"âœ… å·²ç´¢å¼•æ®µè½ {i}/{len(chunk_files)}")
    logging.info("ğŸ‰ å…¨éƒ¨æ®µè½å®Œæˆç´¢å¼•ã€‚\n")


# === å‘é‡æª¢ç´¢å‡½å¼ ===
def search_by_vector(query, size=5):
    query_vec = get_embedding(query)
    if query_vec is None:
        logging.error("âŒ ç„¡æ³•ç”¢ç”ŸæŸ¥è©¢å‘é‡")
        return None

    body = {
        "size": size,
        "query": {
            "script_score": {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "match": {
                                    "content": query
                                }
                            }
                        ]
                    }
                },
                "script": {
                    "source": """
                        double semantic = cosineSimilarity(params.query_vector, 'embedding') + 1.0;
                        double keyword = _score; 
                        return 0.7 * semantic + 0.3 * keyword;
                    """,
                    "params": {
                        "query_vector": query_vec
                    }
                }
            }
        }
    }

    res = es.search(index=ES_INDEX, body=body)
    hits = res["hits"]["hits"]

    logging.info(f"ğŸ” æ··åˆæª¢ç´¢ã€Œ{query}ã€çµæœï¼š")
    for i, hit in enumerate(hits, 1):
        score = hit["_score"]
        content = hit["_source"].get("content", "")
        logging.info(f"  {i}. åˆ†æ•¸: {score:.4f}ï¼Œå…§å®¹å‰30å­—: {content[:30]}...")
    return hits


# === ä¸­æ–‡é—œéµè©æŠ½å–å‡½å¼
def extract_chinese_keywords(text, top_k=10):
    if not text:
        return []
    try:
        return jieba.analyse.extract_tags(text, topK=top_k)
    except Exception as e:
        logging.error(f"âŒ é—œéµå­—æå–å¤±æ•—ï¼š{e}")
        return []


# å»ºç«‹ç°¡è½‰ç¹è½‰æ›å™¨
cc = opencc.OpenCC('s2t')


def to_traditional(text):
    return cc.convert(text)


# === æ”¹è‰¯ç‰ˆé—œéµè©åŒ¹é… ===
def contains_keywords_loose(answer, context, match_threshold=0.6):
    answer = to_traditional(answer)
    context = to_traditional(context)

    ans_clean = re.sub(r'\s+', '', answer.lower())
    ctx_clean = re.sub(r'\s+', '', context.lower())
    ans_clean = re.sub(r'[^\w\u4e00-\u9fff]', '', ans_clean)
    ctx_clean = re.sub(r'[^\w\u4e00-\u9fff]', '', ctx_clean)

    tokens = re.findall(r'[\u4e00-\u9fff]+', ans_clean)
    match_count = sum(1 for t in tokens if t and t in ctx_clean)

    en_tokens = re.findall(r'[A-Za-z0-9]+', ans_clean)
    en_match_count = sum(1 for t in en_tokens if t and t in ctx_clean)

    total_tokens = len(tokens) + len(en_tokens)
    if total_tokens == 0:
        return False

    return (match_count + en_match_count) / total_tokens >= match_threshold


# === ä½¿ç”¨ Gemma æ‘˜è¦å–®æ®µæ–‡å­—ï¼ˆ150Â±10å­—ï¼Œè‡ªç„¶æ–·å¥ç‰ˆï¼‰ ===
def summarize_text(text):
    if not text:
        return ""

    prompt = f"è«‹ç”¨ç°¡æ½”ä¸­æ–‡æ‘˜è¦ä»¥ä¸‹æ®µè½ï¼Œæ§åˆ¶åœ¨140~160å­—ä¹‹é–“ï¼Œä¸”ä¸è¦ä¸­æ–·å¥å­ï¼š\n{text}"

    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": "gemma3:1b", "prompt": prompt},
            stream=True
        )

        full_answer = ""
        for line in response.iter_lines():
            if line:
                try:
                    data = json.loads(line.decode("utf-8"))
                    full_answer += data.get("response", "")
                except:
                    continue

        summary = full_answer.strip()
        if len(summary) > 160:
            cut_pos = summary.rfind("ã€‚", 140, 160)
            if cut_pos == -1:
                cut_pos = summary.rfind("ï¼Œ", 140, 160)
            if cut_pos == -1:
                cut_pos = 150
            summary = summary[:cut_pos].rstrip("ï¼Œã€‚")

    except Exception as e:
        logging.error(f"âŒ æ‘˜è¦å¤±æ•—ï¼š{e}")
        return ""

    if len(summary) < 140 and summary != "":
        prompt2 = f"è«‹è£œå……å‰›å‰›çš„æ‘˜è¦ï¼Œä½¿å…§å®¹æ›´å®Œæ•´ï¼Œä»ç„¶ä¿æŒç°¡æ½”ä¸”é•·åº¦æ¥è¿‘150å­—ï¼Œä¸è¦ä¸­æ–·å¥å­ï¼š\n{summary}"
        try:
            response2 = requests.post(
                "http://localhost:11434/api/generate",
                json={"model": "gemma3:1b", "prompt": prompt2},
                stream=True
            )
            full_answer2 = ""
            for line in response2.iter_lines():
                if line:
                    try:
                        data2 = json.loads(line.decode("utf-8"))
                        full_answer2 += data2.get("response", "")
                    except:
                        continue
            summary2 = full_answer2.strip()
            if len(summary2) > 160:
                cut_pos = summary2.rfind("ã€‚", 140, 160)
                if cut_pos == -1:
                    cut_pos = summary2.rfind("ï¼Œ", 140, 160)
                if cut_pos == -1:
                    cut_pos = 150
                summary2 = summary2[:cut_pos].rstrip("ï¼Œã€‚")
            if len(summary2) > len(summary):
                summary = summary2
        except Exception as e:
            logging.error(f"âŒ è£œå……æ‘˜è¦å¤±æ•—ï¼š{e}")

    return summary


# === å¤šæ®µæ‘˜è¦åˆä½µï¼ˆä¿ç•™æœªæ”¹ï¼‰===
def summarize_multiple_hits(hits, top_n=3):
    summaries = []
    for hit in hits[:top_n]:
        content = hit.get("_source", {}).get("content", "")
        if content:
            summary = summarize_text(content)
            if summary:
                summaries.append(summary)
    return "\n".join(summaries)


# === ä½¿ç”¨ Gemma å›ç­”å•é¡Œ ===
def ask_gemma(question, context):
    context = context or ""

    prompt = f"""ä½ æ˜¯ä¸€ä½åš´è¬¹çš„ç§˜æ›¸å®¤å•ç­”åŠ©æ‰‹ï¼Œ
åªèƒ½æ ¹æ“šæä¾›çš„æ–‡ä»¶å…§å®¹å›ç­”å•é¡Œã€‚
è‹¥æ–‡ä»¶ä¸­æ²’æœ‰æ˜ç¢ºæåˆ°å•é¡Œçš„ç­”æ¡ˆï¼Œè«‹ç›´æ¥å›ç­”ã€ŒæŸ¥ç„¡ç›¸é—œè³‡æ–™ã€ï¼Œä¸è¦çŒœæ¸¬æˆ–è™›æ§‹ç­”æ¡ˆã€‚
ä»¥ä¸‹æ˜¯åƒè€ƒæ–‡ä»¶å…§å®¹ï¼š
========
{context}
========

è«‹å›ç­”ä¸‹é¢é€™å€‹å•é¡Œï¼š
ã€Œ{question}ã€
å›ç­”è¦æ±‚ï¼š
- åªèƒ½æ ¹æ“šä¸Šé¢æ–‡ä»¶å…§å®¹ä½œç­”
- è‹¥æ‰¾ä¸åˆ°æ˜ç¢ºç­”æ¡ˆï¼Œç›´æ¥å›ç­”ã€ŒæŸ¥ç„¡ç›¸é—œè³‡æ–™ã€
- ä¸è¦åŠ å…¥å€‹äººæ¨æ¸¬ã€æ„è¦‹æˆ–é¡å¤–èªªæ˜
- å›ç­”å‹™å¿…ç°¡çŸ­æ‰¼è¦ï¼Œå­—æ•¸è«‹åš´æ ¼é™åˆ¶ 50 å­—ä»¥å…§

"""

    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": "gemma3:1b", "prompt": prompt},
            stream=True
        )

        full_answer = ""
        for line in response.iter_lines():
            if line:
                try:
                    data = json.loads(line.decode("utf-8"))
                    chunk = data.get("response", "")
                    full_answer += chunk
                except Exception:
                    continue

        full_answer = full_answer.strip()
        if len(full_answer) > 50:
            cut_pos = full_answer.find("ã€‚", 40, 60)
            if cut_pos == -1:
                cut_pos = full_answer.find("ï¼Œ", 40, 60)
            if cut_pos != -1:
                full_answer = full_answer[:cut_pos + 1]
            else:
                full_answer = full_answer[:60]

        if not full_answer or full_answer == "":
            final_answer = "ä¸å¥½æ„æ€ï¼Œè«‹æ‚¨æ›´å…·é«”æå•"
        elif full_answer == "æŸ¥ç„¡ç›¸é—œè³‡æ–™":
            final_answer = "æŸ¥ç„¡ç›¸é—œè³‡æ–™"
        else:
            final_answer = full_answer

        logging.info("\nğŸ¤– æ¨¡å‹å›ç­”ï¼š%s", full_answer)
        logging.info("ğŸ’¡ æœ€çµ‚å›ç­”ï¼š%s", final_answer)
        return final_answer

    except Exception as e:
        logging.error(f"âŒ Gemma å›ç­”éŒ¯èª¤ï¼š{e}")
        return "ä¸å¥½æ„æ€ï¼Œè«‹æ‚¨æ›´å…·é«”"


# === ä¸»èŠå¤©è¿´åœˆï¼ˆå–®æ®µæ‘˜è¦ç‰ˆï¼‰===
def chat_loop():
    print("æ­¡è¿ä½¿ç”¨æ±å³å¤§å­¸ç§˜æ›¸å®¤å°å¹«æ‰‹ï¼Œè«‹è¼¸å…¥å•é¡Œï¼Œè¼¸å…¥ q é›¢é–‹ã€‚\n")
    while True:
        question = input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ (q é›¢é–‹): ").strip()
        if question.lower() == 'q':
            print("å†è¦‹ï¼")
            break
        if not question:
            continue

        hits = search_by_vector(question, size=3)
        if not hits:
            print("æŸ¥ç„¡ç›¸é—œè³‡æ–™ï¼Œä¸å¥½æ„æ€ï¼Œè«‹æ‚¨æ›´å…·é«”æå•ã€‚")
            continue

        # åªæ‘˜è¦åˆ†æ•¸æœ€é«˜çš„ç¬¬ä¸€ç­†æ®µè½å…§å®¹
        top_hit = hits[0]
        top_content = top_hit["_source"].get("content", "")
        summarized_context = summarize_text(top_content)

        answer = ask_gemma(question, summarized_context)
        print(f"\nã€ç§˜æ›¸å®¤å›è¦†ã€‘\n{answer}\n")


# === ä¸»æµç¨‹ ===
def main():
    download_files()
    classify_and_extract()
    chunk_texts()
    create_es_index()
    embed_and_index()
    chat_loop()


if __name__ == "__main__":
    main()
